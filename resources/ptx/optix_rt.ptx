//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_50
.address_size 64

	// .globl	__intersection__cylinder

.visible .entry __intersection__cylinder()
{
	.reg .pred 	%p<34>;
	.reg .b16 	%rs<8>;
	.reg .f32 	%f<100>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<4>;


	// begin inline asm
	call (%rd2), _optix_get_sbt_data_ptr_64, ();
	// end inline asm
	ld.u64 	%rd3, [%rd2+8];
	// begin inline asm
	call (%f17), _optix_get_object_ray_origin_x, ();
	// end inline asm
	// begin inline asm
	call (%f18), _optix_get_object_ray_origin_y, ();
	// end inline asm
	// begin inline asm
	call (%f19), _optix_get_object_ray_origin_z, ();
	// end inline asm
	// begin inline asm
	call (%f20), _optix_get_object_ray_direction_x, ();
	// end inline asm
	// begin inline asm
	call (%f21), _optix_get_object_ray_direction_y, ();
	// end inline asm
	// begin inline asm
	call (%f22), _optix_get_object_ray_direction_z, ();
	// end inline asm
	// begin inline asm
	call (%f23), _optix_get_ray_tmin, ();
	// end inline asm
	// begin inline asm
	call (%f24), _optix_get_ray_tmax, ();
	// end inline asm
	add.s64 	%rd1, %rd3, 32;
	ld.v4.f32 	{%f27, %f28, %f29, %f30}, [%rd3+32];
	mul.f32 	%f35, %f27, %f17;
	fma.rn.f32 	%f36, %f28, %f18, %f35;
	fma.rn.f32 	%f37, %f29, %f19, %f36;
	mov.f32 	%f38, 0f3F800000;
	fma.rn.f32 	%f39, %f30, %f38, %f37;
	ld.v4.f32 	{%f40, %f41, %f42, %f43}, [%rd3+48];
	mul.f32 	%f48, %f40, %f17;
	fma.rn.f32 	%f49, %f41, %f18, %f48;
	fma.rn.f32 	%f50, %f42, %f19, %f49;
	fma.rn.f32 	%f51, %f43, %f38, %f50;
	ld.v4.f32 	{%f52, %f53, %f54, %f55}, [%rd3+64];
	mul.f32 	%f60, %f52, %f17;
	fma.rn.f32 	%f61, %f53, %f18, %f60;
	fma.rn.f32 	%f62, %f54, %f19, %f61;
	fma.rn.f32 	%f3, %f55, %f38, %f62;
	mul.f32 	%f63, %f27, %f20;
	fma.rn.f32 	%f64, %f28, %f21, %f63;
	fma.rn.f32 	%f65, %f29, %f22, %f64;
	mov.f32 	%f66, 0f00000000;
	fma.rn.f32 	%f67, %f30, %f66, %f65;
	mul.f32 	%f68, %f40, %f20;
	fma.rn.f32 	%f69, %f41, %f21, %f68;
	fma.rn.f32 	%f70, %f42, %f22, %f69;
	fma.rn.f32 	%f71, %f43, %f66, %f70;
	mul.f32 	%f72, %f52, %f20;
	fma.rn.f32 	%f73, %f53, %f21, %f72;
	fma.rn.f32 	%f74, %f54, %f22, %f73;
	fma.rn.f32 	%f4, %f55, %f66, %f74;
	mul.f32 	%f75, %f71, %f71;
	fma.rn.f32 	%f5, %f67, %f67, %f75;
	mul.f32 	%f76, %f51, %f71;
	fma.rn.f32 	%f77, %f39, %f67, %f76;
	add.f32 	%f6, %f77, %f77;
	mul.f32 	%f78, %f51, %f51;
	fma.rn.f32 	%f79, %f39, %f39, %f78;
	ld.f32 	%f80, [%rd3+164];
	mul.f32 	%f81, %f80, %f80;
	sub.f32 	%f7, %f79, %f81;
	setp.eq.f32 	%p3, %f5, 0f00000000;
	setp.eq.f32 	%p4, %f6, 0f00000000;
	and.pred  	%p5, %p3, %p4;
	mov.pred 	%p33, 0;
	@%p5 bra 	$L__BB0_3;

	neg.f32 	%f82, %f7;
	div.rn.f32 	%f97, %f82, %f6;
	mul.f32 	%f83, %f5, 0fC0800000;
	mul.f32 	%f84, %f83, %f7;
	fma.rn.f32 	%f9, %f6, %f6, %f84;
	setp.neu.f32 	%p7, %f5, 0f00000000;
	setp.lt.f32 	%p8, %f9, 0f00000000;
	and.pred  	%p9, %p8, %p7;
	mov.f32 	%f98, %f97;
	@%p9 bra 	$L__BB0_3;

	mov.b32 	%r1, %f6;
	and.b32  	%r2, %r1, -2147483648;
	sqrt.rn.f32 	%f85, %f9;
	mov.b32 	%r3, %f85;
	and.b32  	%r4, %r3, 2147483647;
	or.b32  	%r5, %r4, %r2;
	mov.b32 	%f86, %r5;
	add.f32 	%f87, %f6, %f86;
	mul.f32 	%f88, %f87, 0fBF000000;
	div.rn.f32 	%f89, %f88, %f5;
	div.rn.f32 	%f90, %f7, %f88;
	min.f32 	%f91, %f89, %f90;
	max.f32 	%f92, %f89, %f90;
	selp.f32 	%f10, %f97, %f91, %p3;
	selp.f32 	%f98, %f97, %f92, %p3;
	mov.pred 	%p33, -1;
	mov.f32 	%f97, %f10;

$L__BB0_3:
	fma.rn.f32 	%f14, %f4, %f97, %f3;
	fma.rn.f32 	%f15, %f4, %f98, %f3;
	setp.ge.f32 	%p12, %f98, %f23;
	setp.le.f32 	%p13, %f97, %f24;
	and.pred  	%p14, %p13, %p12;
	and.pred  	%p15, %p33, %p14;
	setp.leu.f32 	%p16, %f98, %f24;
	setp.geu.f32 	%p17, %f97, %f23;
	or.pred  	%p18, %p17, %p16;
	and.pred  	%p19, %p18, %p15;
	mov.u16 	%rs3, 0;
	not.pred 	%p20, %p19;
	mov.u16 	%rs7, %rs3;
	@%p20 bra 	$L__BB0_9;

	setp.ltu.f32 	%p21, %f14, 0f00000000;
	@%p21 bra 	$L__BB0_6;

	ld.f32 	%f93, [%rd1+128];
	setp.le.f32 	%p22, %f14, %f93;
	setp.gt.f32 	%p23, %f97, %f23;
	and.pred  	%p24, %p23, %p22;
	mov.u16 	%rs7, 1;
	@%p24 bra 	$L__BB0_9;

$L__BB0_6:
	setp.ltu.f32 	%p25, %f15, 0f00000000;
	mov.u16 	%rs7, %rs3;
	@%p25 bra 	$L__BB0_9;

	ld.f32 	%f94, [%rd1+128];
	setp.gtu.f32 	%p26, %f15, %f94;
	mov.u16 	%rs7, %rs3;
	@%p26 bra 	$L__BB0_9;

	setp.lt.f32 	%p27, %f98, %f24;
	selp.u16 	%rs7, 1, 0, %p27;

$L__BB0_9:
	setp.ltu.f32 	%p28, %f14, 0f00000000;
	@%p28 bra 	$L__BB0_11;

	ld.f32 	%f95, [%rd1+128];
	setp.le.f32 	%p29, %f14, %f95;
	setp.ge.f32 	%p30, %f97, 0f00000000;
	and.pred  	%p31, %p30, %p29;
	@%p31 bra 	$L__BB0_12;

$L__BB0_11:
	mov.f32 	%f97, %f98;

$L__BB0_12:
	setp.eq.s16 	%p32, %rs7, 0;
	@%p32 bra 	$L__BB0_14;

	mov.u32 	%r7, 254;
	// begin inline asm
	call (%r6), _optix_report_intersection_0, (%f97, %r7);
	// end inline asm

$L__BB0_14:
	ret;

}
	// .globl	__intersection__disk
.visible .entry __intersection__disk()
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<67>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<3>;


	// begin inline asm
	call (%rd1), _optix_get_sbt_data_ptr_64, ();
	// end inline asm
	ld.u64 	%rd2, [%rd1+8];
	// begin inline asm
	call (%f4), _optix_get_object_ray_origin_x, ();
	// end inline asm
	// begin inline asm
	call (%f5), _optix_get_object_ray_origin_y, ();
	// end inline asm
	// begin inline asm
	call (%f6), _optix_get_object_ray_origin_z, ();
	// end inline asm
	// begin inline asm
	call (%f7), _optix_get_object_ray_direction_x, ();
	// end inline asm
	// begin inline asm
	call (%f8), _optix_get_object_ray_direction_y, ();
	// end inline asm
	// begin inline asm
	call (%f9), _optix_get_object_ray_direction_z, ();
	// end inline asm
	// begin inline asm
	call (%f10), _optix_get_ray_tmin, ();
	// end inline asm
	// begin inline asm
	call (%f11), _optix_get_ray_tmax, ();
	// end inline asm
	ld.v4.f32 	{%f13, %f14, %f15, %f16}, [%rd2+32];
	mul.f32 	%f21, %f13, %f4;
	fma.rn.f32 	%f22, %f14, %f5, %f21;
	fma.rn.f32 	%f23, %f15, %f6, %f22;
	mov.f32 	%f24, 0f3F800000;
	fma.rn.f32 	%f25, %f16, %f24, %f23;
	ld.v4.f32 	{%f26, %f27, %f28, %f29}, [%rd2+48];
	mul.f32 	%f34, %f26, %f4;
	fma.rn.f32 	%f35, %f27, %f5, %f34;
	fma.rn.f32 	%f36, %f28, %f6, %f35;
	fma.rn.f32 	%f37, %f29, %f24, %f36;
	ld.v4.f32 	{%f38, %f39, %f40, %f41}, [%rd2+64];
	mul.f32 	%f46, %f38, %f4;
	fma.rn.f32 	%f47, %f39, %f5, %f46;
	fma.rn.f32 	%f48, %f40, %f6, %f47;
	fma.rn.f32 	%f49, %f41, %f24, %f48;
	mul.f32 	%f50, %f13, %f7;
	fma.rn.f32 	%f51, %f14, %f8, %f50;
	fma.rn.f32 	%f52, %f15, %f9, %f51;
	mov.f32 	%f53, 0f00000000;
	fma.rn.f32 	%f54, %f16, %f53, %f52;
	mul.f32 	%f55, %f26, %f7;
	fma.rn.f32 	%f56, %f27, %f8, %f55;
	fma.rn.f32 	%f57, %f28, %f9, %f56;
	fma.rn.f32 	%f58, %f29, %f53, %f57;
	mul.f32 	%f59, %f38, %f7;
	fma.rn.f32 	%f60, %f39, %f8, %f59;
	fma.rn.f32 	%f61, %f40, %f9, %f60;
	fma.rn.f32 	%f62, %f41, %f53, %f61;
	neg.f32 	%f63, %f49;
	div.rn.f32 	%f1, %f63, %f62;
	fma.rn.f32 	%f2, %f1, %f54, %f25;
	fma.rn.f32 	%f3, %f1, %f58, %f37;
	mul.f32 	%f64, %f3, %f3;
	fma.rn.f32 	%f65, %f2, %f2, %f64;
	setp.gtu.f32 	%p1, %f65, 0f3F800000;
	setp.leu.f32 	%p2, %f1, %f10;
	or.pred  	%p3, %p2, %p1;
	setp.geu.f32 	%p4, %f1, %f11;
	or.pred  	%p5, %p4, %p3;
	@%p5 bra 	$L__BB1_2;

	mov.b32 	%r3, %f2;
	mov.b32 	%r4, %f3;
	mov.u32 	%r2, 254;
	// begin inline asm
	call (%r1), _optix_report_intersection_2, (%f1, %r2, %r3, %r4);
	// end inline asm

$L__BB1_2:
	ret;

}
	// .globl	__intersection__rectangle
.visible .entry __intersection__rectangle()
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<69>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<3>;


	// begin inline asm
	call (%rd1), _optix_get_sbt_data_ptr_64, ();
	// end inline asm
	ld.u64 	%rd2, [%rd1+8];
	// begin inline asm
	call (%f6), _optix_get_object_ray_origin_x, ();
	// end inline asm
	// begin inline asm
	call (%f7), _optix_get_object_ray_origin_y, ();
	// end inline asm
	// begin inline asm
	call (%f8), _optix_get_object_ray_origin_z, ();
	// end inline asm
	// begin inline asm
	call (%f9), _optix_get_object_ray_direction_x, ();
	// end inline asm
	// begin inline asm
	call (%f10), _optix_get_object_ray_direction_y, ();
	// end inline asm
	// begin inline asm
	call (%f11), _optix_get_object_ray_direction_z, ();
	// end inline asm
	// begin inline asm
	call (%f12), _optix_get_ray_tmin, ();
	// end inline asm
	// begin inline asm
	call (%f13), _optix_get_ray_tmax, ();
	// end inline asm
	ld.v4.f32 	{%f15, %f16, %f17, %f18}, [%rd2+32];
	mul.f32 	%f23, %f15, %f6;
	fma.rn.f32 	%f24, %f16, %f7, %f23;
	fma.rn.f32 	%f25, %f17, %f8, %f24;
	mov.f32 	%f26, 0f3F800000;
	fma.rn.f32 	%f27, %f18, %f26, %f25;
	ld.v4.f32 	{%f28, %f29, %f30, %f31}, [%rd2+48];
	mul.f32 	%f36, %f28, %f6;
	fma.rn.f32 	%f37, %f29, %f7, %f36;
	fma.rn.f32 	%f38, %f30, %f8, %f37;
	fma.rn.f32 	%f39, %f31, %f26, %f38;
	ld.v4.f32 	{%f40, %f41, %f42, %f43}, [%rd2+64];
	mul.f32 	%f48, %f40, %f6;
	fma.rn.f32 	%f49, %f41, %f7, %f48;
	fma.rn.f32 	%f50, %f42, %f8, %f49;
	fma.rn.f32 	%f51, %f43, %f26, %f50;
	mul.f32 	%f52, %f15, %f9;
	fma.rn.f32 	%f53, %f16, %f10, %f52;
	fma.rn.f32 	%f54, %f17, %f11, %f53;
	mov.f32 	%f55, 0f00000000;
	fma.rn.f32 	%f56, %f18, %f55, %f54;
	mul.f32 	%f57, %f28, %f9;
	fma.rn.f32 	%f58, %f29, %f10, %f57;
	fma.rn.f32 	%f59, %f30, %f11, %f58;
	fma.rn.f32 	%f60, %f31, %f55, %f59;
	mul.f32 	%f61, %f40, %f9;
	fma.rn.f32 	%f62, %f41, %f10, %f61;
	fma.rn.f32 	%f63, %f42, %f11, %f62;
	fma.rn.f32 	%f64, %f43, %f55, %f63;
	neg.f32 	%f65, %f51;
	div.rn.f32 	%f3, %f65, %f64;
	fma.rn.f32 	%f4, %f3, %f56, %f27;
	fma.rn.f32 	%f5, %f3, %f60, %f39;
	abs.f32 	%f66, %f4;
	setp.gtu.f32 	%p1, %f66, 0f3F800000;
	@%p1 bra 	$L__BB2_3;

	abs.f32 	%f67, %f5;
	setp.gtu.f32 	%p2, %f67, 0f3F800000;
	setp.leu.f32 	%p3, %f3, %f12;
	or.pred  	%p4, %p2, %p3;
	setp.geu.f32 	%p5, %f3, %f13;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	$L__BB2_3;

	mov.b32 	%r3, %f4;
	mov.b32 	%r4, %f5;
	mov.u32 	%r2, 254;
	// begin inline asm
	call (%r1), _optix_report_intersection_2, (%f3, %r2, %r3, %r4);
	// end inline asm

$L__BB2_3:
	ret;

}
	// .globl	__intersection__sdfgrid
.visible .entry __intersection__sdfgrid()
{
	.reg .pred 	%p<88>;
	.reg .f32 	%f<316>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<23>;


	// begin inline asm
	call (%rd2), _optix_get_sbt_data_ptr_64, ();
	// end inline asm
	ld.u64 	%rd1, [%rd2+8];
	ld.u64 	%rd3, [%rd1];
	// begin inline asm
	call (%r8), _optix_read_primitive_idx, ();
	// end inline asm
	mul.wide.u32 	%rd4, %r8, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.u32 	%r9, [%rd5];
	// begin inline asm
	call (%f76), _optix_get_object_ray_origin_x, ();
	// end inline asm
	// begin inline asm
	call (%f77), _optix_get_object_ray_origin_y, ();
	// end inline asm
	// begin inline asm
	call (%f78), _optix_get_object_ray_origin_z, ();
	// end inline asm
	// begin inline asm
	call (%f79), _optix_get_object_ray_direction_x, ();
	// end inline asm
	// begin inline asm
	call (%f80), _optix_get_object_ray_direction_y, ();
	// end inline asm
	// begin inline asm
	call (%f81), _optix_get_object_ray_direction_z, ();
	// end inline asm
	// begin inline asm
	call (%f82), _optix_get_ray_tmin, ();
	// end inline asm
	// begin inline asm
	call (%f83), _optix_get_ray_tmax, ();
	// end inline asm
	ld.v4.f32 	{%f85, %f86, %f87, %f88}, [%rd1+48];
	mul.f32 	%f93, %f85, %f76;
	fma.rn.f32 	%f94, %f86, %f77, %f93;
	fma.rn.f32 	%f95, %f87, %f78, %f94;
	mov.f32 	%f96, 0f3F800000;
	fma.rn.f32 	%f3, %f88, %f96, %f95;
	ld.v4.f32 	{%f97, %f98, %f99, %f100}, [%rd1+64];
	mul.f32 	%f105, %f97, %f76;
	fma.rn.f32 	%f106, %f98, %f77, %f105;
	fma.rn.f32 	%f107, %f99, %f78, %f106;
	fma.rn.f32 	%f4, %f100, %f96, %f107;
	ld.v4.f32 	{%f108, %f109, %f110, %f111}, [%rd1+80];
	mul.f32 	%f116, %f108, %f76;
	fma.rn.f32 	%f117, %f109, %f77, %f116;
	fma.rn.f32 	%f118, %f110, %f78, %f117;
	fma.rn.f32 	%f5, %f111, %f96, %f118;
	mul.f32 	%f119, %f85, %f79;
	fma.rn.f32 	%f120, %f86, %f80, %f119;
	fma.rn.f32 	%f121, %f87, %f81, %f120;
	mov.f32 	%f122, 0f00000000;
	fma.rn.f32 	%f6, %f88, %f122, %f121;
	mul.f32 	%f123, %f97, %f79;
	fma.rn.f32 	%f124, %f98, %f80, %f123;
	fma.rn.f32 	%f125, %f99, %f81, %f124;
	fma.rn.f32 	%f7, %f100, %f122, %f125;
	mul.f32 	%f126, %f108, %f79;
	fma.rn.f32 	%f127, %f109, %f80, %f126;
	fma.rn.f32 	%f128, %f110, %f81, %f127;
	fma.rn.f32 	%f8, %f111, %f122, %f128;
	ld.v2.u32 	{%r10, %r11}, [%rd1+8];
	add.s32 	%r12, %r10, -1;
	rem.u32 	%r3, %r9, %r12;
	sub.s32 	%r13, %r9, %r3;
	add.s32 	%r14, %r11, -1;
	div.u32 	%r15, %r13, %r14;
	rem.u32 	%r4, %r15, %r14;
	mul.lo.s32 	%r16, %r4, %r12;
	sub.s32 	%r17, %r13, %r16;
	mul.lo.s32 	%r18, %r14, %r12;
	div.u32 	%r5, %r17, %r18;
	cvt.rn.f32.u32 	%f9, %r3;
	cvt.rn.f32.u32 	%f10, %r4;
	cvt.rn.f32.u32 	%f11, %r5;
	add.f32 	%f129, %f9, 0f3F800000;
	add.f32 	%f130, %f10, 0f3F800000;
	ld.v2.f32 	{%f131, %f132}, [%rd1+24];
	ld.f32 	%f133, [%rd1+20];
	mul.f32 	%f134, %f133, %f9;
	mul.f32 	%f135, %f131, %f10;
	mul.f32 	%f136, %f133, %f129;
	mul.f32 	%f137, %f131, %f130;
	rcp.rn.f32 	%f138, %f6;
	rcp.rn.f32 	%f139, %f7;
	setp.ltu.f32 	%p6, %f138, 0f00000000;
	selp.f32 	%f140, %f136, %f134, %p6;
	sub.f32 	%f141, %f140, %f3;
	mul.f32 	%f298, %f138, %f141;
	selp.f32 	%f142, %f134, %f136, %p6;
	sub.f32 	%f143, %f142, %f3;
	mul.f32 	%f297, %f138, %f143;
	setp.ltu.f32 	%p7, %f139, 0f00000000;
	selp.f32 	%f144, %f137, %f135, %p7;
	sub.f32 	%f145, %f144, %f4;
	mul.f32 	%f16, %f139, %f145;
	selp.f32 	%f146, %f135, %f137, %p7;
	sub.f32 	%f147, %f146, %f4;
	mul.f32 	%f17, %f139, %f147;
	setp.gt.f32 	%p8, %f298, %f17;
	setp.gt.f32 	%p9, %f16, %f297;
	or.pred  	%p10, %p8, %p9;
	mov.pred 	%p84, -1;
	@%p10 bra 	$L__BB3_14;

	setp.gt.f32 	%p11, %f16, %f298;
	@%p11 bra 	$L__BB3_3;

	abs.f32 	%f148, %f298;
	setp.lt.f32 	%p12, %f148, 0f7F800000;
	@%p12 bra 	$L__BB3_4;

$L__BB3_3:
	mov.f32 	%f298, %f16;

$L__BB3_4:
	setp.lt.f32 	%p13, %f17, %f297;
	@%p13 bra 	$L__BB3_6;

	abs.f32 	%f149, %f297;
	setp.lt.f32 	%p14, %f149, 0f7F800000;
	@%p14 bra 	$L__BB3_7;

$L__BB3_6:
	mov.f32 	%f297, %f17;

$L__BB3_7:
	mul.f32 	%f150, %f132, %f11;
	add.f32 	%f151, %f11, 0f3F800000;
	mul.f32 	%f152, %f132, %f151;
	rcp.rn.f32 	%f153, %f8;
	setp.ltu.f32 	%p16, %f153, 0f00000000;
	selp.f32 	%f154, %f152, %f150, %p16;
	sub.f32 	%f155, %f154, %f5;
	mul.f32 	%f20, %f153, %f155;
	selp.f32 	%f156, %f150, %f152, %p16;
	sub.f32 	%f157, %f156, %f5;
	mul.f32 	%f21, %f153, %f157;
	setp.gt.f32 	%p17, %f298, %f21;
	setp.gt.f32 	%p18, %f20, %f297;
	or.pred  	%p19, %p17, %p18;
	@%p19 bra 	$L__BB3_14;

	setp.gt.f32 	%p20, %f20, %f298;
	@%p20 bra 	$L__BB3_10;

	abs.f32 	%f158, %f298;
	setp.lt.f32 	%p21, %f158, 0f7F800000;
	@%p21 bra 	$L__BB3_11;

$L__BB3_10:
	mov.f32 	%f298, %f20;

$L__BB3_11:
	setp.lt.f32 	%p22, %f21, %f297;
	@%p22 bra 	$L__BB3_13;

	abs.f32 	%f159, %f297;
	setp.lt.f32 	%p24, %f159, 0f7F800000;
	mov.pred 	%p84, 0;
	@%p24 bra 	$L__BB3_14;

$L__BB3_13:
	mov.pred 	%p84, 0;
	mov.f32 	%f297, %f21;

$L__BB3_14:
	@%p84 bra 	$L__BB3_37;

	max.f32 	%f25, %f298, %f82;
	setp.lt.f32 	%p26, %f297, %f25;
	@%p26 bra 	$L__BB3_37;

	cvt.rn.f32.u32 	%f160, %r12;
	cvt.rn.f32.u32 	%f161, %r14;
	ld.u32 	%r21, [%rd1+16];
	add.s32 	%r22, %r21, -1;
	cvt.rn.f32.u32 	%f162, %r22;
	mul.f32 	%f163, %f3, %f160;
	mov.f32 	%f301, 0f00000000;
	fma.rn.f32 	%f165, %f301, %f4, %f163;
	fma.rn.f32 	%f166, %f301, %f5, %f165;
	neg.f32 	%f167, %f9;
	mov.f32 	%f168, 0f3F800000;
	fma.rn.f32 	%f169, %f167, %f168, %f166;
	mul.f32 	%f170, %f3, 0f00000000;
	fma.rn.f32 	%f171, %f161, %f4, %f170;
	fma.rn.f32 	%f172, %f301, %f5, %f171;
	neg.f32 	%f173, %f10;
	fma.rn.f32 	%f174, %f173, %f168, %f172;
	fma.rn.f32 	%f175, %f301, %f4, %f170;
	fma.rn.f32 	%f176, %f162, %f5, %f175;
	neg.f32 	%f177, %f11;
	fma.rn.f32 	%f178, %f177, %f168, %f176;
	mul.f32 	%f179, %f6, %f160;
	fma.rn.f32 	%f180, %f301, %f7, %f179;
	fma.rn.f32 	%f181, %f301, %f8, %f180;
	fma.rn.f32 	%f182, %f167, %f301, %f181;
	mul.f32 	%f183, %f6, 0f00000000;
	fma.rn.f32 	%f184, %f161, %f7, %f183;
	fma.rn.f32 	%f185, %f301, %f8, %f184;
	fma.rn.f32 	%f186, %f173, %f301, %f185;
	fma.rn.f32 	%f187, %f301, %f7, %f183;
	fma.rn.f32 	%f188, %f162, %f8, %f187;
	fma.rn.f32 	%f189, %f177, %f301, %f188;
	mul.lo.s32 	%r23, %r11, %r5;
	add.s32 	%r24, %r4, %r23;
	mul.lo.s32 	%r25, %r24, %r10;
	add.s32 	%r26, %r25, %r3;
	ld.u64 	%rd6, [%rd1+32];
	mul.wide.u32 	%rd7, %r26, 4;
	add.s64 	%rd8, %rd6, %rd7;
	add.s32 	%r27, %r3, 1;
	add.s32 	%r28, %r25, %r27;
	mul.wide.u32 	%rd9, %r28, 4;
	add.s64 	%rd10, %rd6, %rd9;
	add.s32 	%r29, %r4, 1;
	add.s32 	%r30, %r23, %r29;
	mul.lo.s32 	%r31, %r30, %r10;
	add.s32 	%r32, %r31, %r3;
	mul.wide.u32 	%rd11, %r32, 4;
	add.s64 	%rd12, %rd6, %rd11;
	add.s32 	%r33, %r31, %r27;
	mul.wide.u32 	%rd13, %r33, 4;
	add.s64 	%rd14, %rd6, %rd13;
	add.s32 	%r34, %r23, %r11;
	add.s32 	%r35, %r34, %r4;
	mul.lo.s32 	%r36, %r35, %r10;
	add.s32 	%r37, %r36, %r3;
	mul.wide.u32 	%rd15, %r37, 4;
	add.s64 	%rd16, %rd6, %rd15;
	add.s32 	%r38, %r36, %r27;
	mul.wide.u32 	%rd17, %r38, 4;
	add.s64 	%rd18, %rd6, %rd17;
	add.s32 	%r39, %r34, %r29;
	mul.lo.s32 	%r40, %r39, %r10;
	add.s32 	%r41, %r40, %r3;
	mul.wide.u32 	%rd19, %r41, 4;
	add.s64 	%rd20, %rd6, %rd19;
	add.s32 	%r42, %r40, %r27;
	mul.wide.u32 	%rd21, %r42, 4;
	add.s64 	%rd22, %rd6, %rd21;
	fma.rn.f32 	%f190, %f25, %f182, %f169;
	fma.rn.f32 	%f191, %f25, %f186, %f174;
	fma.rn.f32 	%f192, %f25, %f189, %f178;
	ld.f32 	%f193, [%rd18];
	ld.f32 	%f194, [%rd16];
	sub.f32 	%f195, %f193, %f194;
	ld.f32 	%f196, [%rd10];
	ld.f32 	%f197, [%rd8];
	sub.f32 	%f198, %f196, %f197;
	ld.f32 	%f199, [%rd12];
	sub.f32 	%f200, %f199, %f197;
	ld.f32 	%f201, [%rd14];
	sub.f32 	%f202, %f201, %f199;
	sub.f32 	%f203, %f202, %f198;
	sub.f32 	%f204, %f197, %f194;
	sub.f32 	%f205, %f198, %f195;
	ld.f32 	%f206, [%rd20];
	sub.f32 	%f207, %f206, %f194;
	sub.f32 	%f208, %f200, %f207;
	ld.f32 	%f209, [%rd22];
	sub.f32 	%f210, %f209, %f206;
	sub.f32 	%f211, %f210, %f195;
	sub.f32 	%f212, %f203, %f211;
	mul.f32 	%f213, %f190, %f191;
	mul.f32 	%f214, %f182, %f186;
	mul.f32 	%f215, %f191, %f182;
	fma.rn.f32 	%f216, %f190, %f186, %f215;
	neg.f32 	%f217, %f198;
	fma.rn.f32 	%f218, %f205, %f192, %f217;
	neg.f32 	%f219, %f200;
	fma.rn.f32 	%f220, %f208, %f192, %f219;
	neg.f32 	%f221, %f203;
	fma.rn.f32 	%f222, %f212, %f192, %f221;
	neg.f32 	%f223, %f197;
	fma.rn.f32 	%f224, %f204, %f192, %f223;
	mul.f32 	%f225, %f213, %f222;
	fma.rn.f32 	%f226, %f191, %f220, %f225;
	fma.rn.f32 	%f227, %f190, %f218, %f226;
	add.f32 	%f26, %f224, %f227;
	mul.f32 	%f228, %f186, %f220;
	fma.rn.f32 	%f229, %f182, %f218, %f228;
	fma.rn.f32 	%f230, %f216, %f222, %f229;
	mul.f32 	%f231, %f212, %f213;
	fma.rn.f32 	%f232, %f208, %f191, %f231;
	fma.rn.f32 	%f233, %f205, %f190, %f232;
	add.f32 	%f234, %f204, %f233;
	fma.rn.f32 	%f27, %f189, %f234, %f230;
	mul.f32 	%f235, %f212, %f216;
	fma.rn.f32 	%f236, %f208, %f186, %f235;
	fma.rn.f32 	%f237, %f205, %f182, %f236;
	mul.f32 	%f238, %f189, %f237;
	fma.rn.f32 	%f28, %f214, %f222, %f238;
	mul.f32 	%f239, %f212, %f214;
	mul.f32 	%f29, %f189, %f239;
	sub.f32 	%f30, %f297, %f25;
	setp.neu.f32 	%p27, %f29, 0f00000000;
	@%p27 bra 	$L__BB3_24;
	bra.uni 	$L__BB3_17;

$L__BB3_24:
	mul.f32 	%f39, %f29, 0f40400000;
	setp.eq.f32 	%p52, %f39, 0f00000000;
	add.f32 	%f40, %f28, %f28;
	setp.eq.f32 	%p53, %f40, 0f00000000;
	and.pred  	%p54, %p52, %p53;
	mov.pred 	%p86, -1;
	mov.f32 	%f302, %f301;
	@%p54 bra 	$L__BB3_27;

	neg.f32 	%f256, %f27;
	div.rn.f32 	%f301, %f256, %f40;
	mul.f32 	%f257, %f39, 0fC0800000;
	mul.f32 	%f258, %f257, %f27;
	fma.rn.f32 	%f42, %f40, %f40, %f258;
	setp.neu.f32 	%p56, %f39, 0f00000000;
	setp.lt.f32 	%p57, %f42, 0f00000000;
	and.pred  	%p58, %p57, %p56;
	mov.f32 	%f302, %f301;
	@%p58 bra 	$L__BB3_27;

	mov.b32 	%r52, %f40;
	and.b32  	%r53, %r52, -2147483648;
	sqrt.rn.f32 	%f259, %f42;
	mov.b32 	%r54, %f259;
	and.b32  	%r55, %r54, 2147483647;
	or.b32  	%r56, %r55, %r53;
	mov.b32 	%f260, %r56;
	add.f32 	%f261, %f40, %f260;
	mul.f32 	%f262, %f261, 0fBF000000;
	div.rn.f32 	%f263, %f262, %f39;
	div.rn.f32 	%f264, %f27, %f262;
	min.f32 	%f265, %f263, %f264;
	max.f32 	%f266, %f263, %f264;
	selp.f32 	%f302, %f301, %f265, %p52;
	selp.f32 	%f301, %f301, %f266, %p52;
	mov.pred 	%p86, 0;

$L__BB3_27:
	mov.f32 	%f315, 0f00000000;
	fma.rn.f32 	%f268, %f29, %f315, %f28;
	fma.rn.f32 	%f269, %f268, %f315, %f27;
	fma.rn.f32 	%f308, %f269, %f315, %f26;
	fma.rn.f32 	%f270, %f29, %f30, %f28;
	fma.rn.f32 	%f271, %f270, %f30, %f27;
	fma.rn.f32 	%f310, %f271, %f30, %f26;
	mov.f32 	%f307, %f315;
	mov.f32 	%f309, %f30;
	@%p86 bra 	$L__BB3_32;

	setp.ltu.f32 	%p61, %f302, 0f00000000;
	mov.f32 	%f307, 0f00000000;
	setp.gtu.f32 	%p62, %f302, %f30;
	or.pred  	%p63, %p61, %p62;
	mov.f32 	%f309, %f30;
	@%p63 bra 	$L__BB3_30;

	fma.rn.f32 	%f273, %f29, %f302, %f28;
	fma.rn.f32 	%f274, %f273, %f302, %f27;
	fma.rn.f32 	%f275, %f274, %f302, %f26;
	mul.f32 	%f276, %f308, %f275;
	setp.le.f32 	%p64, %f276, 0f00000000;
	selp.f32 	%f307, 0f00000000, %f302, %p64;
	selp.f32 	%f308, %f308, %f275, %p64;
	selp.f32 	%f309, %f302, %f30, %p64;
	selp.f32 	%f310, %f275, %f310, %p64;

$L__BB3_30:
	setp.gtu.f32 	%p65, %f301, %f309;
	setp.gtu.f32 	%p66, %f307, %f301;
	or.pred  	%p67, %p66, %p65;
	@%p67 bra 	$L__BB3_32;

	fma.rn.f32 	%f277, %f29, %f301, %f28;
	fma.rn.f32 	%f278, %f277, %f301, %f27;
	fma.rn.f32 	%f279, %f278, %f301, %f26;
	mul.f32 	%f280, %f308, %f279;
	setp.le.f32 	%p68, %f280, 0f00000000;
	selp.f32 	%f307, %f307, %f301, %p68;
	selp.f32 	%f308, %f308, %f279, %p68;
	selp.f32 	%f309, %f301, %f309, %p68;
	selp.f32 	%f310, %f279, %f310, %p68;

$L__BB3_32:
	mul.f32 	%f282, %f308, %f310;
	setp.gt.f32 	%p70, %f282, 0f00000000;
	mov.pred 	%p87, 0;
	@%p70 bra 	$L__BB3_35;

	mov.u32 	%r61, 0;

$L__BB3_34:
	sub.f32 	%f283, %f310, %f308;
	div.rn.f32 	%f284, %f308, %f283;
	sub.f32 	%f285, %f309, %f307;
	mul.f32 	%f286, %f284, %f285;
	sub.f32 	%f315, %f307, %f286;
	fma.rn.f32 	%f287, %f29, %f315, %f28;
	fma.rn.f32 	%f288, %f287, %f315, %f27;
	fma.rn.f32 	%f289, %f288, %f315, %f26;
	mul.f32 	%f290, %f308, %f289;
	setp.le.f32 	%p72, %f290, 0f00000000;
	selp.f32 	%f310, %f289, %f310, %p72;
	selp.f32 	%f308, %f308, %f289, %p72;
	selp.f32 	%f309, %f315, %f309, %p72;
	selp.f32 	%f307, %f307, %f315, %p72;
	sub.f32 	%f291, %f309, %f307;
	abs.f32 	%f292, %f291;
	setp.geu.f32 	%p73, %f292, 0f3727C5AC;
	setp.lt.f32 	%p74, %f292, 0f3727C5AC;
	add.s32 	%r58, %r61, 1;
	setp.lt.u32 	%p75, %r58, 51;
	selp.b32 	%r61, %r61, %r58, %p74;
	mov.pred 	%p87, -1;
	and.pred  	%p76, %p75, %p73;
	@%p76 bra 	$L__BB3_34;

$L__BB3_35:
	setp.ltu.f32 	%p77, %f315, 0f00000000;
	not.pred 	%p78, %p87;
	or.pred  	%p79, %p77, %p78;
	setp.gtu.f32 	%p80, %f315, %f30;
	or.pred  	%p81, %p80, %p79;
	add.f32 	%f75, %f25, %f315;
	setp.geu.f32 	%p82, %f75, %f83;
	or.pred  	%p83, %p81, %p82;
	@%p83 bra 	$L__BB3_37;

	mov.u32 	%r60, 254;
	// begin inline asm
	call (%r59), _optix_report_intersection_0, (%f75, %r60);
	// end inline asm
	bra.uni 	$L__BB3_37;

$L__BB3_17:
	setp.eq.f32 	%p29, %f28, 0f00000000;
	setp.eq.f32 	%p30, %f27, 0f00000000;
	and.pred  	%p31, %p29, %p30;
	mov.pred 	%p85, 0;
	@%p31 bra 	$L__BB3_20;

	neg.f32 	%f241, %f26;
	div.rn.f32 	%f299, %f241, %f27;
	mul.f32 	%f242, %f28, 0fC0800000;
	mul.f32 	%f243, %f26, %f242;
	fma.rn.f32 	%f32, %f27, %f27, %f243;
	setp.neu.f32 	%p33, %f28, 0f00000000;
	setp.lt.f32 	%p34, %f32, 0f00000000;
	and.pred  	%p35, %p34, %p33;
	mov.f32 	%f300, %f299;
	@%p35 bra 	$L__BB3_20;

	mov.b32 	%r43, %f27;
	and.b32  	%r44, %r43, -2147483648;
	sqrt.rn.f32 	%f244, %f32;
	mov.b32 	%r45, %f244;
	and.b32  	%r46, %r45, 2147483647;
	or.b32  	%r47, %r46, %r44;
	mov.b32 	%f245, %r47;
	add.f32 	%f246, %f27, %f245;
	mul.f32 	%f247, %f246, 0fBF000000;
	div.rn.f32 	%f248, %f247, %f28;
	div.rn.f32 	%f249, %f26, %f247;
	min.f32 	%f250, %f248, %f249;
	max.f32 	%f251, %f248, %f249;
	selp.f32 	%f33, %f299, %f250, %p29;
	selp.f32 	%f300, %f299, %f251, %p29;
	mov.pred 	%p85, -1;
	mov.f32 	%f299, %f33;

$L__BB3_20:
	setp.ge.f32 	%p38, %f299, 0f00000000;
	and.pred  	%p39, %p38, %p85;
	setp.le.f32 	%p40, %f299, %f30;
	and.pred  	%p41, %p40, %p39;
	add.f32 	%f37, %f25, %f299;
	setp.lt.f32 	%p42, %f37, %f83;
	and.pred  	%p43, %p41, %p42;
	@%p43 bra 	$L__BB3_23;
	bra.uni 	$L__BB3_21;

$L__BB3_23:
	mov.u32 	%r51, 254;
	// begin inline asm
	call (%r50), _optix_report_intersection_0, (%f37, %r51);
	// end inline asm
	bra.uni 	$L__BB3_37;

$L__BB3_21:
	setp.ltu.f32 	%p44, %f300, 0f00000000;
	not.pred 	%p45, %p85;
	or.pred  	%p46, %p44, %p45;
	setp.gtu.f32 	%p47, %f300, %f30;
	or.pred  	%p48, %p47, %p46;
	add.f32 	%f38, %f25, %f300;
	setp.geu.f32 	%p49, %f38, %f83;
	or.pred  	%p50, %p48, %p49;
	@%p50 bra 	$L__BB3_37;

	mov.u32 	%r49, 254;
	// begin inline asm
	call (%r48), _optix_report_intersection_0, (%f38, %r49);
	// end inline asm

$L__BB3_37:
	ret;

}
	// .globl	__intersection__sphere
.visible .entry __intersection__sphere()
{
	.reg .pred 	%p<30>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<4>;


	// begin inline asm
	call (%rd2), _optix_get_sbt_data_ptr_64, ();
	// end inline asm
	ld.u64 	%rd3, [%rd2+8];
	// begin inline asm
	call (%f28), _optix_get_object_ray_origin_x, ();
	// end inline asm
	// begin inline asm
	call (%f29), _optix_get_object_ray_origin_y, ();
	// end inline asm
	// begin inline asm
	call (%f30), _optix_get_object_ray_origin_z, ();
	// end inline asm
	// begin inline asm
	call (%f31), _optix_get_object_ray_direction_x, ();
	// end inline asm
	// begin inline asm
	call (%f32), _optix_get_object_ray_direction_y, ();
	// end inline asm
	// begin inline asm
	call (%f33), _optix_get_object_ray_direction_z, ();
	// end inline asm
	// begin inline asm
	call (%f34), _optix_get_ray_tmin, ();
	// end inline asm
	// begin inline asm
	call (%f35), _optix_get_ray_tmax, ();
	// end inline asm
	add.s64 	%rd1, %rd3, 32;
	ld.f32 	%f9, [%rd3+32];
	sub.f32 	%f37, %f28, %f9;
	ld.f32 	%f10, [%rd3+36];
	sub.f32 	%f38, %f29, %f10;
	ld.f32 	%f11, [%rd3+40];
	sub.f32 	%f39, %f30, %f11;
	neg.f32 	%f40, %f38;
	neg.f32 	%f41, %f39;
	mul.f32 	%f42, %f37, %f31;
	neg.f32 	%f43, %f42;
	fma.rn.f32 	%f44, %f40, %f32, %f43;
	fma.rn.f32 	%f45, %f41, %f33, %f44;
	mul.f32 	%f46, %f31, %f31;
	fma.rn.f32 	%f47, %f32, %f32, %f46;
	fma.rn.f32 	%f12, %f33, %f33, %f47;
	sqrt.rn.f32 	%f48, %f12;
	div.rn.f32 	%f13, %f45, %f48;
	setp.neu.f32 	%p2, %f13, 0f00000000;
	@%p2 bra 	$L__BB4_2;

	setp.neu.f32 	%p3, %f28, %f9;
	setp.neu.f32 	%p4, %f29, %f10;
	or.pred  	%p5, %p3, %p4;
	setp.neu.f32 	%p6, %f30, %f11;
	or.pred  	%p7, %p5, %p6;
	@%p7 bra 	$L__BB4_8;

$L__BB4_2:
	fma.rn.f32 	%f49, %f13, %f31, %f28;
	sub.f32 	%f14, %f49, %f9;
	fma.rn.f32 	%f50, %f13, %f32, %f29;
	sub.f32 	%f15, %f50, %f10;
	fma.rn.f32 	%f51, %f13, %f33, %f30;
	sub.f32 	%f16, %f51, %f11;
	mul.f32 	%f52, %f14, %f14;
	fma.rn.f32 	%f53, %f15, %f15, %f52;
	fma.rn.f32 	%f17, %f16, %f16, %f53;
	sqrt.rn.f32 	%f54, %f17;
	ld.f32 	%f18, [%rd1+16];
	setp.gt.f32 	%p8, %f54, %f18;
	@%p8 bra 	$L__BB4_8;

	mul.f32 	%f56, %f14, %f31;
	fma.rn.f32 	%f57, %f15, %f32, %f56;
	fma.rn.f32 	%f58, %f16, %f33, %f57;
	add.f32 	%f19, %f58, %f58;
	mul.f32 	%f59, %f18, %f18;
	sub.f32 	%f20, %f17, %f59;
	setp.eq.f32 	%p10, %f19, 0f00000000;
	setp.eq.f32 	%p11, %f12, 0f00000000;
	and.pred  	%p12, %p11, %p10;
	mov.pred 	%p29, 0;
	@%p12 bra 	$L__BB4_6;

	neg.f32 	%f60, %f20;
	div.rn.f32 	%f74, %f60, %f19;
	mul.f32 	%f61, %f12, 0fC0800000;
	mul.f32 	%f62, %f61, %f20;
	fma.rn.f32 	%f22, %f19, %f19, %f62;
	setp.neu.f32 	%p14, %f12, 0f00000000;
	setp.lt.f32 	%p15, %f22, 0f00000000;
	and.pred  	%p16, %p15, %p14;
	mov.f32 	%f75, %f74;
	@%p16 bra 	$L__BB4_6;

	mov.b32 	%r1, %f19;
	and.b32  	%r2, %r1, -2147483648;
	sqrt.rn.f32 	%f63, %f22;
	mov.b32 	%r3, %f63;
	and.b32  	%r4, %r3, 2147483647;
	or.b32  	%r5, %r4, %r2;
	mov.b32 	%f64, %r5;
	add.f32 	%f65, %f19, %f64;
	mul.f32 	%f66, %f65, 0fBF000000;
	div.rn.f32 	%f67, %f66, %f12;
	div.rn.f32 	%f68, %f20, %f66;
	min.f32 	%f69, %f67, %f68;
	max.f32 	%f70, %f67, %f68;
	selp.f32 	%f23, %f74, %f69, %p11;
	selp.f32 	%f75, %f74, %f70, %p11;
	mov.pred 	%p29, -1;
	mov.f32 	%f74, %f23;

$L__BB4_6:
	add.f32 	%f71, %f13, %f74;
	setp.le.f32 	%p19, %f71, %f35;
	add.f32 	%f72, %f13, %f75;
	setp.ge.f32 	%p20, %f72, 0f00000000;
	setp.geu.f32 	%p21, %f71, %f34;
	setp.leu.f32 	%p22, %f72, %f35;
	setp.lt.f32 	%p23, %f71, 0f00000000;
	selp.f32 	%f27, %f72, %f71, %p23;
	and.pred  	%p24, %p20, %p19;
	and.pred  	%p25, %p29, %p24;
	or.pred  	%p26, %p22, %p21;
	and.pred  	%p27, %p25, %p26;
	not.pred 	%p28, %p27;
	@%p28 bra 	$L__BB4_8;

	mov.u32 	%r7, 254;
	// begin inline asm
	call (%r6), _optix_report_intersection_0, (%f27, %r7);
	// end inline asm

$L__BB4_8:
	ret;

}

