{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53b58d7-4a4c-41cd-b689-c42462d7c464",
   "metadata": {},
   "source": [
    "# Volumetric inverse rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00d2cc-3e22-464c-bf13-cf703e29cf23",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, we use Mitsuba's differentiable volumetric path tracer to optimize a scattering volume to match a set of (synthetic) reference images. We will optimize a 3D volume density that's stored on a regular grid. The optimization will account for both direct and indirect illumination by using [path replay backpropagation][1] to compute derivatives of delta tracking and volumetric multiple scattering. The reconstructed volume parameters can then for example be re-rendered using novel illumination conditions.\n",
    "\n",
    "\n",
    "<div class=\"admonition important alert alert-block alert-success\">\n",
    "\n",
    "ðŸš€ **You will learn how to:**\n",
    "    \n",
    "<ul>\n",
    "  <li>Construct a scene with volumes</li>\n",
    "  <li>Optimize a volume grid to match a set of reference images</li>\n",
    "  <li>Upscale the optimized parameters during the optimization</li>\n",
    "</ul>\n",
    "    \n",
    "</div>\n",
    "\n",
    "[1]: https://rgl.epfl.ch/publications/Vicini2021PathReplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644bbd28-7d4b-4494-8926-d8a1e892d69d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As always, we start with the usual imports and set a variant that supports automatic differentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec80cc-d2a5-46da-83bd-cd66f8947a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "root = os.path.abspath(\"\")\n",
    "sys.path.insert(0, f\"{root}/build/python\")\n",
    "sys.path.insert(0, f\"{root}/src/python/python/loaders\")\n",
    "\n",
    "import drjit as dr\n",
    "import mitsuba as mi\n",
    "\n",
    "mi.set_variant('llvm_ad_rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd1cb4f-c4e5-4bf7-9b24-417c4ee080a9",
   "metadata": {},
   "source": [
    "## Creating multiple sensors\n",
    "\n",
    "We cannot hope to obtain a robust volumetric reconstruction using only a single reference image. Multiple viewpoints are needed to sufficiently constrain the reconstructed volume density.  Using a multi-view optimization we can recover volume parameters that generalize to novel views (and illumination conditions).\n",
    "\n",
    "In this tutorial, we use 5 sensors placed on a half circle around the origin. For the simple optimization in this tutorial this is sufficient, but more complex scenes may require using significantly more views (e.g., using 50-100 sensors is not unreasonable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10333b5e-eb4a-4b97-bb97-7895f2d8cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mitsuba import ScalarTransform4f as T\n",
    "\n",
    "sensor_count = 5\n",
    "sensors_dict = {\n",
    "    'type': 'batch',\n",
    "    'film': {\n",
    "            'type': 'hdrfilm',\n",
    "            'width': 64 * sensor_count, 'height': 64,\n",
    "            'filter': {'type': 'tent'}\n",
    "    },\n",
    "}\n",
    "\n",
    "for i in range(sensor_count):\n",
    "    angle = 180.0 / sensor_count * i - 90.0\n",
    "    sensor_rotation = T.rotate([0, 1, 0], angle)\n",
    "    sensor_to_world = T.look_at(target=[0, 0, 0], origin=[0, 0, 4], up=[0, 1, 0])\n",
    "    sensors_dict['sensor' + str(i)] = {\n",
    "        'type': 'perspective',\n",
    "        'fov': 45,\n",
    "        'to_world': sensor_rotation @ sensor_to_world,\n",
    "    }\n",
    "sensors = mi.load_dict(sensors_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf2808-6b4f-49a8-b3c2-3d390c17e406",
   "metadata": {},
   "source": [
    "## Rendering synthetic reference images\n",
    "\n",
    "We will now setup a simple scene with a constant environment illumination and a reference volume placed at the origin. The heterogenous volume is instantiated inside of a cube. We assign the `null` BSDF to the cube's surface, since we do not want the cube's surface to interact with light in any way (i.e., the surface should be invisible). To learn more about volume rendering in Mitsuba, please refer to the [plugin documentation][1].\n",
    "\n",
    "We then render this scene using the previously created sensors and store the resulting images in a list for later use.\n",
    "\n",
    "\n",
    "[1]: https://mitsuba.readthedocs.io/en/latest/src/generated/plugins_media.html#heterogeneous-medium-heterogeneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d2244-3224-4da5-a453-328920b8c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_dict = {\n",
    "    'type': 'scene',\n",
    "    'integrator': {'type': 'prbvolpath'},\n",
    "    'object': {\n",
    "        'type': 'cube',\n",
    "        'bsdf': {'type': 'null'},\n",
    "        'interior': {\n",
    "            'type': 'heterogeneous',\n",
    "            'sigma_t': {\n",
    "                'type': 'gridvolume',\n",
    "                'filename': './tutorials/scenes/volume.vol',\n",
    "                'to_world': T.rotate([1, 0, 0], -90).scale(2).translate(-0.5)\n",
    "            },\n",
    "            'scale': 40\n",
    "        }\n",
    "    },\n",
    "    'emitter': {'type': 'constant'}\n",
    "}\n",
    "\n",
    "scene_ref = mi.load_dict(scene_dict)\n",
    "\n",
    "# Number of samples per pixel for reference images\n",
    "ref_spp = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd5b53a-5c17-437a-83d2-6712bbfe1d86",
   "metadata": {
    "nbsphinx": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IGNORE THIS: When running under pytest, adjust parameters to reduce computation time\n",
    "import os\n",
    "if 'PYTEST_CURRENT_TEST' in os.environ:\n",
    "    ref_spp = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16008559-86a3-4b9f-80e2-c28ccb5aac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_images = mi.render(scene_ref, sensor=sensors, spp=ref_spp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d1dd5-6693-4030-a942-35ff70b7b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mi.util.convert_to_bitmap(ref_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f10c4-265e-4f53-9515-7688b142c2db",
   "metadata": {},
   "source": [
    "## Setting up the optimization scene\n",
    "Our goal is now to optimize a 3D volume density (also called extinction) to match the previously generated reference images. For this we create a second scene, where we replace the reference volume by a simple uniform initialization. \n",
    "\n",
    "To initialize a volume grid from Python, we use the [<code>VolumeGrid</code>][2] object in conjunction with [<code>TensorXf</code>][3]. The `VolumeGrid` class is responsible for loading and writing volumes from disk, similar to the `Bitmap` class for images. Using the `grid` property of the [<code>gridvolume</code>][4] plugin, it is possible to pass it directly to the plugin constructor in Python.\n",
    "\n",
    "We initialize the extinction `sigma_t` to a low constant value, (e.g. `0.002`). This tends to help the optimization process, as it seems to be easier for the optimizer to increase the volume density rather than remove parts of a very dense volume. \n",
    "\n",
    "Note that we use a fairly small initial volume resolution here. This is done on purpose since we will upsample the volume grid during the actual optimization process. As explained later, this typically improves the convexity of the volume optimization problem.\n",
    "\n",
    "[1]: https://mitsuba.readthedocs.io/en/latest/src/generated/plugins_media.html#heterogeneous-medium-heterogeneous\n",
    "[2]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.VolumeGrid\n",
    "[3]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.TensorXf\n",
    "[4]: https://mitsuba.readthedocs.io/en/latest/src/generated/plugins_media.html#grid-based-volume-data-source-gridvolume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82313a97-a314-43a3-8a4b-f4e32305f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_res = 16\n",
    "\n",
    "# Modify the scene dictionary\n",
    "scene_dict['object'] = {\n",
    "    'type': 'cube',\n",
    "    'interior': {\n",
    "        'type': 'heterogeneous',\n",
    "        'sigma_t': {\n",
    "            'type': 'gridvolume',\n",
    "            'grid': mi.VolumeGrid(dr.full(mi.TensorXf, 0.002, (v_res, v_res, v_res, 1))),\n",
    "            'to_world': T.translate(-1).scale(2.0)\n",
    "        },\n",
    "        'scale': 40.0,\n",
    "    },\n",
    "    'bsdf': {'type': 'null'}\n",
    "}\n",
    "\n",
    "scene = mi.load_dict(scene_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890a714-a81c-4f37-bc9f-7ec2e4335a6a",
   "metadata": {},
   "source": [
    "We load the modified scene and render it for all view angles. Those are going to be our initial image in the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91050ab4-91ac-413b-89c0-959d024498e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_images = mi.render(scene, sensor=sensors, spp=ref_spp)\n",
    "mi.util.convert_to_bitmap(init_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1cf830-9ad3-4bd7-98ca-3840e3179378",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "We instantiate an `Adam` optimizer and load the `sigma_t` grid data as parameter to be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4e964-c5f9-4d7d-81d3-e8669bf153c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = mi.traverse(scene)\n",
    "\n",
    "key = 'object.interior_medium.sigma_t.data'\n",
    "\n",
    "opt = mi.ad.Adam(lr=0.02)\n",
    "opt[key] = params[key]\n",
    "params.update(opt);\n",
    "\n",
    "iteration_count = 40\n",
    "spp = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d312f-e883-4e37-ad42-b93c360e51fd",
   "metadata": {
    "nbsphinx": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IGNORE THIS: When running under pytest, adjust parameters to reduce computation time\n",
    "import os\n",
    "if 'PYTEST_CURRENT_TEST' in os.environ:\n",
    "    iteration_count = 2\n",
    "    spp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210af9ef-b720-4d60-842a-88daa316db9f",
   "metadata": {},
   "source": [
    "We then run the optimization loop for a few iterations, similar to the other tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af17b50-c809-472e-a55d-4b1589a5e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(iteration_count):\n",
    "    total_loss = 0.0\n",
    "    # Perform the differentiable light transport simulation\n",
    "    img = mi.render(scene, params, sensor=sensors, spp=spp, seed=it)\n",
    "    \n",
    "    # L2 loss function\n",
    "    loss = dr.mean(dr.sqr(img - ref_images))\n",
    "    \n",
    "    # Backpropagate gradients\n",
    "    dr.backward(loss)\n",
    "\n",
    "    # Take a gradient step\n",
    "    opt.step()\n",
    "    \n",
    "    # Clamp the optimized density values. Since we used the `scale` parameter \n",
    "    # when instantiating the volume, we are in fact optimizing extinction \n",
    "    # in a range from [1e-6 * scale, scale].\n",
    "    opt[key] = dr.clamp(opt[key], 1e-6, 1.0)\n",
    "    \n",
    "    # Propagate changes to the scene\n",
    "    params.update(opt)\n",
    "    \n",
    "    total_loss += loss[0]\n",
    "    print(f\"Iteration {it:02d}: error={total_loss:6f}\", end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f046cdc9-572e-43a1-b281-3ae65a29097e",
   "metadata": {},
   "source": [
    "## Intermediate results\n",
    "\n",
    "We have only performed a few iterations so far and can take a look at the current results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c3bf2f-21f7-4635-8fce-261a70cd5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_images = mi.render(scene, sensor=sensors, spp=ref_spp)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(14, 4))\n",
    "plt.imshow(mi.util.convert_to_bitmap(intermediate_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c597b6e9-ca45-441b-ab6d-9bea4f35d329",
   "metadata": {},
   "source": [
    "## Volume upsampling\n",
    "\n",
    "The results above don't look great. One reason is the low resolution of the optimized volume grid. Let's try to increase the resolution of the current grid and continue the optimization for a few more iterations. In practice it is almost always beneficial to leverage such a \"multi-resolution\" approach. At low resolution, the optimization will recover the overall shape, exploring a much simpler solution landscape. Moving on to a volume with a higher resolution allows recovering additional detail, while using the coarser solution as a starting point.\n",
    "\n",
    "Luckily Dr.Jit provides [<code>dr.upsample()</code>][1], a functions for up-sampling tensor and texture data. We can easily create a higher resolution volume by passing the current optimzed tensor and specifying the desired shape (must be powers of two when upsampling `TensorXf`).\n",
    "\n",
    "[1]: https://drjit.readthedocs.io/en/latest/reference.html#drjit.upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdfe9de-d1b2-4d06-89ed-ce2d93d1c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt[key] = dr.upsample(opt[key], shape=(64, 64, 64))\n",
    "params.update(opt);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b99e7-fb72-4b3f-b166-2d7930daf86d",
   "metadata": {},
   "source": [
    "Rendering the new, upsampled volume we can already notice a slight difference in the apparent sharpness. This is due to the *trilinear* interpolation of density values that is used by the volumetric path tracer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c5176-ded0-4d13-8996-5d4dbe006c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "upscale_images = [mi.render(scene, sensor=sensors[i], spp=ref_spp) for i in range(sensor_count)]\n",
    "\n",
    "fig, axs = plt.subplots(1, sensor_count, figsize=(14, 4))\n",
    "for i in range(sensor_count):\n",
    "    axs[i].imshow(mi.util.convert_to_bitmap(upscale_images[i]))\n",
    "    axs[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86592e8-a87c-497f-8c94-297cfbe06ad5",
   "metadata": {},
   "source": [
    "## Continuing the optimization\n",
    "\n",
    "Let's now run our optimization loop for a few more iterations with the upscaled volume grid.\n",
    "\n",
    "<div class=\"admonition important alert alert-block alert-info\">\n",
    "\n",
    "ðŸ—’ **Note**\n",
    "    \n",
    "The optimizer automatically resets the internal state (e.g., momentum) associated to the optimized variable when it detects a size change.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b4e1b-a280-4178-8935-dd5c636aa846",
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(iteration_count):\n",
    "    total_loss = 0.0\n",
    "    for sensor_idx in range(sensor_count):\n",
    "        img = mi.render(scene, params, sensor=sensors[sensor_idx], spp=8*spp, seed=it)\n",
    "        loss = dr.mean(dr.sqr(img - ref_images[sensor_idx]))\n",
    "        dr.backward(loss)\n",
    "        opt.step()\n",
    "        opt[key] = dr.clamp(opt[key], 1e-6, 1.0)\n",
    "        params.update(opt)\n",
    "        total_loss += loss[0]\n",
    "    print(f\"Iteration {it:02d}: error={total_loss:6f}\", end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725e9980-f9c5-416a-a1f1-896e5942716d",
   "metadata": {},
   "source": [
    "## Final results\n",
    "\n",
    "Finally we can render the final volume from the different view points and compare the images to the reference images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57383d4d-780f-4824-a679-5021ca6d6a19",
   "metadata": {
    "nbsphinx-thumbnail": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_images = [mi.render(scene, sensor=sensors[i], spp=ref_spp) for i in range(sensor_count)]\n",
    "\n",
    "fig, axs = plt.subplots(2, sensor_count, figsize=(14, 6))\n",
    "for i in range(sensor_count):\n",
    "    axs[0][i].imshow(mi.util.convert_to_bitmap(ref_images[i]))\n",
    "    axs[0][i].axis('off')\n",
    "    axs[1][i].imshow(mi.util.convert_to_bitmap(final_images[i]))\n",
    "    axs[1][i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34cf18c-75c6-4280-a69d-6467023e8cac",
   "metadata": {},
   "source": [
    "Of course the results could be further improved by taking more iterations or adopting other advanced optimization schemes, such as multi-resolution rendering where the rendering resolution is increased throughout the optimization process. Additionally, it can sometimes be beneficial to add a sparsity (e.g., an $L_1$ loss on the density values) or smoothness prior (e.g., a total variation regularizer penalizing differences between neighboring voxels).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4473987-40c1-4d8a-b133-fc8d7e319420",
   "metadata": {},
   "source": [
    "## See also\n",
    "\n",
    "- [<code>prbvolpath</code> plugin](https://mitsuba.readthedocs.io/en/latest/src/generated/plugins_integrators.html#path-replay-backpropagation-volumetric-integrator-prbvolpath)\n",
    "- [<code>heterogeneous</code> plugin](https://mitsuba.readthedocs.io/en/latest/src/generated/plugins_media.html#heterogeneous-medium-heterogeneous)\n",
    "- [<code>gridvolume</code> plugin](https://mitsuba.readthedocs.io/en/latest/src/generated/plugins_media.html#grid-based-volume-data-source-gridvolume)\n",
    "- [<code>mitsuba.VolumeGrid</code>](https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.VolumeGrid)\n",
    "- [<code>mitsuba.TensorXf</code>](https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.TensorXf)\n",
    "- [<code>dr.upsample</code>](https://drjit.readthedocs.io/en/latest/reference.html#drjit.upsample)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afd680236861e4ad68138f9ddf1f8bff806918beb77b7f0c16179efa24869fce"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
